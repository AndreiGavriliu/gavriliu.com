[{"body":"","link":"https://gavriliu.com/","section":"","tags":null,"title":"Andrei Gavriliu"},{"body":"","link":"https://gavriliu.com/tags/ingress/","section":"tags","tags":null,"title":"Ingress"},{"body":"","link":"https://gavriliu.com/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"You've finally tamed Kubernetes Ingress — or so you think. Then weird routing errors, downtime, and mysterious 404s show up. Welcome to the club! 🎩\nLet's save you some headaches. Here are 7 common Kubernetes Ingress mistakes, how they happen, and how to dodge them like a pro.\nQuick Mistakes Checklist Mistake Symptom Quick Fix No Ingress Controller No traffic reaching apps Install a controller (NGINX, Traefik, etc.) Mixing Path and Host Wrong backend routing Match host then path properly Missing PathType Paths don't match Set pathType: Prefix or Exact TLS Failures HTTPS broken Create \u0026amp; reference correct TLS Secret Missing IngressClass Ingress ignored Set spec.ingressClassName Backend Service Issues 502/503 errors Verify Service names, ports, selectors Health Check Problems 504 Gateway Timeout Add /health, tune timeouts Forgetting to Deploy an Ingress Controller Symptom:\nYour Ingress resource is happily created... but no traffic ever reaches your app. Why:\nIngress needs a controller (like NGINX, Traefik, HAProxy). Kubernetes doesn't come with one by default. How to Avoid:\nInstall an Ingress Controller manually. Use something like: 1kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml Make sure it’s running and ready. Pro Tip: If you see no matches for kind \u0026quot;Ingress\u0026quot;, check your controller is alive.\nMixing Up Path vs Host Routing Symptom:\nTraffic goes to the wrong backend. Why:\nForgetting that Ingress matches hosts first, then paths. How to Avoid:\nKnow the flow:\nMatch host Then match path Example:\n1- host: api.example.com 2 path: / 3- host: shop.example.com 4 path: /cart Wrong: expecting /cart at api.example.com to route!\nRight: set both host and path carefully.\nIgnoring PathType Symptom:\nIngress works on some URLs but not others. Why:\nKubernetes requires pathType (since v1.18+). Defaults aren't what you expect. How to Avoid:\nSet your pathType!\n1pathType: Prefix Options:\nPrefix: Match any path starting with /foo Exact: Match exactly /foo ImplementationSpecific: (aka Controller decides — spooky) Prefer Prefix unless you love surprises.\nTLS Configuration Failures Symptom:\nHTTPS connections fail. Browser shows \u0026quot;Connection not secure.\u0026quot; Why:\nMissing or invalid TLS secret. Wrong secret name. Controller not picking it up. How to Avoid:\nCreate your TLS secret: 1kubectl create secret tls my-tls-secret --cert=cert.pem --key=key.pem Reference it properly: 1spec: 2 tls: 3 - hosts: 4 - myapp.example.com 5 secretName: my-tls-secret Checklist:\nSecret in the same namespace as the Ingress! Correct secret name. Controller supports TLS (NGINX, Traefik do). Wrong or Missing IngressClass Symptom:\nIngress resource is ignored. Why:\nKubernetes 1.18+ uses spec.ingressClassName to match Controllers. If it doesn’t match, your Ingress won’t be picked up. How to Avoid:\nSet it explicitly: 1spec: 2 ingressClassName: nginx Match whatever your Controller reports as IngressClass. You can check installed classes:\n1kubectl get ingressclass Incorrect Backend Service Configuration Symptom:\nIngress routes traffic but you get 502/503 errors. Why:\nIngress is fine, but the Service points to nowhere. Wrong service name or port. How to Avoid:\nDouble-check backend services. Example:\n1backend: 2 service: 3 name: api-service 4 port: 5 number: 80 Service must exist. Service must target Pods correctly (selectors right). Debug Tip:\nCheck your service endpoints: 1kubectl get endpoints If it's empty, your Pods don't match. Overlooking Health Checks Symptom:\nRandom 504 Gateway Timeouts. Why:\nIngress Controllers perform health checks on Services. If your app doesn’t respond fast enough, it’s marked unhealthy. How to Avoid:\nMake sure your app is ready! Expose health endpoints (/health, /readyz) Tune Controller's timeout settings via annotations: Example for NGINX:\n1metadata: 2 annotations: 3 nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;30\u0026#34; 4 nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;30\u0026#34; Bonus: Pro Moves Version mismatch: Check your Ingress resource version (networking.k8s.io/v1)! Custom error pages: Many Controllers allow custom 404/503 pages. Nice for UX. Rate limiting: Protect your services from abuse with Controller-specific options. Wrapping Up Kubernetes Ingress is powerful but a little... temperamental. 😉\nAvoid these 7 common mistakes and your cluster will reward you with:\nFaster deployments Cleaner networking Happier users (and fewer 2 a.m. calls) You got this!\n","link":"https://gavriliu.com/post/2025/05/13/kubernetes-ingress-7-common-mistakes-and-how-to-avoid-them/","section":"post","tags":["kubernetes","learn","ingress"],"title":"Kubernetes Ingress: 7 Common Mistakes (and How to Avoid Them)"},{"body":"","link":"https://gavriliu.com/tags/learn/","section":"tags","tags":null,"title":"Learn"},{"body":"","link":"https://gavriliu.com/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://gavriliu.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Your app is running in Kubernetes. Great! But now you want users to actually reach it — without remembering 15 different NodePorts or manually messing with LoadBalancers. Enter the hero we need but rarely understand at first: Ingress.\nToday, we’re breaking down what Ingresses are, how they work, and how they can route traffic like air traffic controllers on espresso.\nWhat Is an Ingress? An Ingress is a Kubernetes resource that manages external access to your services, typically over HTTP(S).\nIt acts as:\nA smart router based on URLs or hostnames A TLS terminator (offloading HTTPS) A single point for path or domain-based routing And the best part? One external IP can serve many services.\nIngress vs Ingress Controller You need to know two key pieces:\nThing What It Does Ingress Resource The rules (\u0026quot;route /api to service A, /shop to B\u0026quot;) Ingress Controller The software that enforces those rules (NGINX, Traefik, etc.) An Ingress by itself does nothing. It's like writing a menu without a waiter. You need an Ingress Controller deployed in your cluster to make it actually work.\nA Basic Ingress Example 1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: my-ingress 5spec: 6 ingressClassName: nginx 7 rules: 8 - host: myapp.example.com 9 http: 10 paths: 11 - path: /api 12 pathType: Prefix 13 backend: 14 service: 15 name: api-service 16 port: 17 number: 80 18 - path: /shop 19 pathType: Prefix 20 backend: 21 service: 22 name: shop-service 23 port: 24 number: 80 What’s happening here?\nRequests to myapp.example.com/api get routed to api-service Requests to myapp.example.com/shop get routed to shop-service All through a single Ingress endpoint!\nPath-Based vs Host-Based Routing Path-based routing = one domain, many paths:\n/api /shop /blog Host-based routing = multiple domains:\n1- host: api.example.com 2- host: shop.example.com Use whichever fits your app’s vibe — or mix both like a networking ninja.\nAdding TLS (HTTPS) Kubernetes Ingress can also handle TLS termination:\n1spec: 2 tls: 3 - hosts: 4 - myapp.example.com 5 secretName: my-tls-secret my-tls-secret contains your TLS certificate and key. Now your users get that sweet, sweet padlock in the browser. (You’re welcome, compliance team.)\nAnnotations: Special Sauce for Ingress Controllers Each Ingress Controller may support annotations to customize behavior.\nExample for NGINX:\n1metadata: 2 annotations: 3 nginx.ingress.kubernetes.io/rewrite-target: / Other cool things you can configure:\nRequest timeouts Connection limits IP whitelisting Authentication Each controller has its own \u0026quot;dialect,\u0026quot; so check their docs!\nWhat About IngressClass? With modern Kubernetes (v1.18+), you should use the spec.ingressClassName field to declare which Ingress Controller handles your Ingress.\n1spec: 2 ingressClassName: nginx The old way using annotations (kubernetes.io/ingress.class: \u0026quot;nginx\u0026quot;) is deprecated!\nStick with ingressClassName to be future-proof.\nPopular Ingress Controllers NGINX Ingress Controller: The classic Traefik: Easy, modern, has cool dashboards HAProxy Ingress: For the performance purists Istio Gateway: If you’ve gone full service mesh (you brave soul) Each has pros, cons, and their own tuning knobs.\nQuick Visual Recap 1User --\u0026gt; Ingress Controller --\u0026gt; Ingress Rules --\u0026gt; Service --\u0026gt; Pod Ingress Controller listens on a public IP It reads Ingress resources Routes traffic according to the rules Simple. Beautiful. Sometimes frustrating. (But mostly beautiful.)\nIngress Is Great For: Consolidating public endpoints HTTPS termination Path-based or hostname-based routing Reducing LoadBalancers in the cloud ($$$) Ingress Is Not Always the Best Choice For: Situation Why Ingress Might Not Be Ideal What to Use Instead Non-HTTP/S traffic Ingress is designed only for HTTP and HTTPS. It can’t handle raw TCP, UDP, gRPC (without extra config) directly. Use a Service (type: LoadBalancer or NodePort), or an Ingress Controller with TCP/UDP support (like Traefik). Ultra-low-latency needs Ingress adds an extra hop: User → LoadBalancer → Ingress → Service → Pod. For very latency-sensitive apps (e.g., real-time games, financial apps), this can be too slow. Direct LoadBalancer Services (Service.type: LoadBalancer) or HostNetwork deployments. Quick external exposure of non-HTTP/S If you just want to quickly expose a database, SSH, MQTT broker, or other TCP/UDP service, Ingress won't help. Use Service.type: LoadBalancer if your cloud provider supports it, or manually assign externalIPs on a Service. Very simple local dev clusters Setting up Ingress Controllers locally (like on minikube, kind) can be extra work. Sometimes a NodePort is easier. Use Service.type: NodePort in dev for fast local access. Quick Tip About Services NodePort: Opens a port on every node. Access at \u0026lt;node IP\u0026gt;:\u0026lt;nodePort\u0026gt;. LoadBalancer: Automatically gets a public IP (in the cloud). ExternalIPs: Manually expose services (common in bare metal setups). HostNetwork: Pods share the host's network stack. Careful with security! Wrapping It Up Ingress is the Kubernetes way to bring order to the networking chaos. It:\nRoutes traffic like a boss Terminates TLS like a pro Consolidates access points Saves you from spinning up 47 LoadBalancers and explaining your cloud bill Just remember: Ingress resources need a Controller. Choose wisely. Configure thoughtfully. And maybe add a TLS certificate so your users don't panic.\nBecause when your apps are live, your routing rules shouldn’t just exist — they should slay.\n","link":"https://gavriliu.com/post/2025/05/06/kubernetes-ingress-your-clusters-traffic-director/","section":"post","tags":["kubernetes","learn","ingress"],"title":"Kubernetes Ingress: Your Cluster's Traffic Director"},{"body":"","link":"https://gavriliu.com/series/kubernetes/","section":"series","tags":null,"title":"Kubernetes"},{"body":"So you’ve got Pods doing great things — calculating, serving, storing, or maybe just vibing. But how do they talk to each other, or to the outside world? Enter Kubernetes Services — the built-in matchmakers making sure traffic finds the right Pods without ghosting.\nLet’s unravel all the types of Services, how they work, and when to use which one — in plain speak, with a splash of professional sarcasm.\nWhat Is a Service? A Service in Kubernetes is a stable networking abstraction over a set of Pods. Since Pods can die and respawn with different IPs (like mayflies with faster DevOps), you can’t reliably connect to a Pod directly. Services solve this by:\nGiving you a fixed IP and DNS name Forwarding traffic to the right Pods via selectors Optionally exposing your app outside the cluster The Anatomy of a Service Here's the minimalist version:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: my-service 5spec: 6 selector: 7 app: my-app 8 ports: 9 - protocol: TCP 10 port: 80 11 targetPort: 8080 selector: Finds Pods with label app: my-app port: The port your Service listens on targetPort: Where the traffic actually goes on the Pod 1. ClusterIP: The Introvert Default type. Only accessible inside the cluster.\n1spec: 2 type: ClusterIP Use when:\nYou don’t need external access One app (say, a frontend) talks to another (like an API or database) Think of it like internal company email — efficient, private, and nobody outside knows it exists.\n2. NodePort: The Over-sharer Exposes the Service on a static port on every node’s IP.\n1spec: 2 type: NodePort 3 ports: 4 - port: 80 5 targetPort: 8080 6 nodePort: 30007 Access it via http://\u0026lt;node-ip\u0026gt;:30007.\nUse when:\nYou want a quick way to test your app from outside You don’t have a cloud LoadBalancer Downside:\nHardcoding ports can lead to conflicts Not the prettiest URL It’s like taping your home address to every light pole in town — useful, but not exactly graceful.\n3. LoadBalancer: The VIP Pass (Cloud Only) If your cluster is running in a cloud provider (AWS, GCP, Azure, etc.), this will provision an external load balancer.\n1spec: 2 type: LoadBalancer Access via a cloud-managed IP or hostname.\nUse when:\nYou want external users to reach your service You're in the cloud and want it done the “cloud native” way Kubernetes does all the hard work: it tells your cloud, “Hey, give me a load balancer,” and boom — traffic flows.\n4. ExternalName: The Alias-er Points to an external DNS name. No selector. No pods. Just DNS aliasing.\n1spec: 2 type: ExternalName 3 externalName: my.db.example.com Use when:\nYou want to refer to an external service via internal DNS You need to abstract something not in the cluster No traffic routing here — just DNS voodoo.\n5. Headless Services: The No-Load-Balancer Club When you want no cluster IP — for direct access to individual Pods (e.g. with StatefulSets).\n1spec: 2 clusterIP: None This lets clients resolve A/AAAA records to each Pod IP directly. Ideal for:\nStatefulSets Peer-to-peer apps Custom service discovery It’s like saying, “Don’t give me a receptionist. I’ll talk to the team directly.”\nA Word on Selectors Selectors are how a Service finds which Pods to send traffic to.\n1selector: 2 app: my-api Kubernetes continuously watches for Pods matching this label. No match? No traffic. So label your Pods like your uptime depends on it. (Because it might.)\nHostNetwork: A Not-Service But Still Important When a Pod wants to skip Kubernetes networking and use the node's actual network stack.\n1spec: 2 hostNetwork: true Use when:\nYou need access to host-level ports (like 80/443) You're running something like a CNI plugin or monitoring tool But beware:\nNo port conflict protection Less isolation It’s like renting a room and using the landlord’s Wi-Fi, mailbox, and fridge. Cool, but risky.\nService Discovery: The DNS Magic Kubernetes creates DNS names for Services automatically:\n1my-service.default.svc.cluster.local Your Pods can use this to reach Services without knowing their IP. It’s like calling someone by name instead of memorizing their phone number.\nBonus: Traffic Routing Behind the Scenes Kubernetes uses iptables or IPVS under the hood to handle traffic routing. It ensures load balancing, round-robin-style, and uses kube-proxy to make it all work.\nQuick Comparison Table Type Accessible From Needs Cloud Use Case ClusterIP Inside Cluster No Internal services NodePort External (via node IP) No Dev/test, simple access LoadBalancer External (via LB IP) Yes Public-facing apps ExternalName Inside Cluster No DNS alias to external services Headless Inside Cluster No Stateful apps needing Pod-level DNS HostNetwork Anywhere (via node IP) No Low-level access, infra tools Wrapping It Up Kubernetes Services are like networking assistants on Red Bull:\nThey watch your Pods Match traffic to them Give DNS names, IPs, and external access Let you scale without reconfiguring apps So next time your Pods are lost and lonely, just remember: all they need is a Service to find their happily-ever-after.\n","link":"https://gavriliu.com/post/2025/04/29/kubernetes-services-the-network-matchmakers/","section":"post","tags":["kubernetes","learn","services"],"title":"Kubernetes Services: The Network Matchmakers"},{"body":"","link":"https://gavriliu.com/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://gavriliu.com/tags/services/","section":"tags","tags":null,"title":"Services"},{"body":"You’ve learned about Deployments, and now you’re deploying stateless apps like a boss. But then you hit a wall - maybe it’s a database, a cache, or something that cares about identity and storage. Enter: StatefulSets.\nStatefulSets are like that one friend who always insists on sitting in the same seat - and gets mad if they can't.\nWhat is a StatefulSet? A StatefulSet is a Kubernetes controller used to manage stateful applications. Unlike Deployments, StatefulSets:\nGive each Pod a stable, unique network identity Retain persistent storage even when a Pod is deleted Ensure Pods are created, updated, and deleted in order In short, StatefulSets are what you use when your app:\nCan’t just be randomly killed and restarted Needs to store and retain data Needs stable DNS names to communicate with peer Pods A Real-Life Analogy Deployment: \u0026quot;Hey, you’re a worker. Grab any desk and get to it.\u0026quot; StatefulSet: \u0026quot;You're Dave. You sit at Desk #3. Your coffee mug is there. If you move, your entire routine is ruined.\u0026quot; A Basic StatefulSet Example Here’s a simplified YAML to show what a StatefulSet looks like:\n1apiVersion: apps/v1 2kind: StatefulSet 3metadata: 4 name: redis 5spec: 6 serviceName: \u0026#34;redis\u0026#34; 7 replicas: 3 8 selector: 9 matchLabels: 10 app: redis 11 template: 12 metadata: 13 labels: 14 app: redis 15 spec: 16 containers: 17 - name: redis 18 image: redis:7 19 ports: 20 - containerPort: 6379 21 volumeClaimTemplates: 22 - metadata: 23 name: data 24 spec: 25 accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] 26 resources: 27 requests: 28 storage: 1Gi Let’s unpack this like your app depends on it (because it does):\nreplicas: 3 - We want 3 Pods. But not just any Pods. These will be called: redis-0 redis-1 redis-2 serviceName: redis - Each Pod gets a stable DNS name like redis-0.redis.default.svc.cluster.local. volumeClaimTemplates - Each Pod gets its own PersistentVolumeClaim (PVC), like a personal journal. Data is not shared between Pods. Why the Order Matters With StatefulSets, order is everything:\nPods are created sequentially: redis-0 → redis-1 → redis-2 Pods are deleted in reverse: redis-2 → redis-1 → redis-0 When updating, K8s updates Pods one at a time, waiting for each to be ready This makes StatefulSets ideal for apps that need quorum, coordination, or recovery - think databases like Cassandra, MongoDB, or distributed systems like Zookeeper.\nPersistent Storage: Your Pod’s Sock Drawer With Deployments, Pods are ephemeral - they come and go, and all local data goes with them. StatefulSets, on the other hand, give each Pod a PVC that sticks around.\nEven if the Pod gets deleted and recreated, its storage (and name) remain the same. It’s like saving a named workspace in the cloud instead of scribbling notes on a whiteboard.\nHeadless Services and DNS A StatefulSet needs a headless service to manage DNS for its Pods. Why?\nBecause each Pod gets a DNS name like:\n1redis-0.redis.default.svc.cluster.local You define the headless service like this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis 5spec: 6 clusterIP: None 7 selector: 8 app: redis 9 ports: 10 - port: 6379 Notice the clusterIP: None? That’s what makes it headless - so K8s won’t load balance, and DNS queries return individual Pod IPs, not a single shared one.\nStatefulSet Lifecycle: The Long, Orderly March Let’s say you create a 3-replica StatefulSet.\nHere’s what happens:\nredis-0 is created. Kubernetes waits until it’s ready. redis-1 is created next. Again, K8s waits. Finally, redis-2. If redis-1 fails health checks, redis-2 will not be created. K8s will just sit patiently, tapping its fingers.\nIt’s like assembling IKEA furniture - if Step 2 fails, you don’t even think about Step 3.\nDeleting a StatefulSet: What Stays and What Goes Deleting a StatefulSet does not delete the PVCs by default.\nSo even if your Pods are gone, their data remains - like ghosts in the persistent volume. You can manually clean them up if needed, but Kubernetes is trying to help you avoid accidental data loss.\nWhat StatefulSets Are Great For Databases (Postgres, MySQL, MongoDB) Distributed systems (Kafka, Zookeeper, etcd) Apps that require stable identities Any service where order, identity, and persistent data matter What They’re Not Great For Stateless apps (just use Deployments!) Services that can scale randomly Apps that don’t need individual storage or DNS identity If you try to use StatefulSets for a simple web app, you’re probably over-engineering it. It’s like renting a forklift to carry a sandwich.\nWrapping It Up: The Stateful Life Here’s your tl;dr:\nStatefulSets give your Pods stable identities and persistent storage. Pods are created and deleted in order. Great for stateful apps like databases and clusters. Backed by headless services for DNS magic. PVCs stick around even if Pods don’t. StatefulSets are Kubernetes’ way of saying: “This app has trust issues - let’s treat it with care, give it a fixed name, a personal volume, and never move its stuff without asking.”\n","link":"https://gavriliu.com/post/2025/04/22/kubernetes-statefulsets-because-some-pods-need-to-remember-things/","section":"post","tags":["kubernetes","learn","statefulset"],"title":"Kubernetes StatefulSets: Because Some Pods Need to Remember Things"},{"body":"","link":"https://gavriliu.com/tags/statefulset/","section":"tags","tags":null,"title":"Statefulset"},{"body":"","link":"https://gavriliu.com/tags/deployment/","section":"tags","tags":null,"title":"Deployment"},{"body":"So you’ve heard about Kubernetes and now you’re swimming in YAML like it’s alphabet soup. Let’s break down one of its most useful concepts - the almighty Deployment - and figure out how it works without having to learn an arcane spellbook.\nWhat’s a Deployment, Anyway? A Kubernetes Deployment is like your project manager. It doesn’t do the actual work (that's what Pods are for), but it makes sure your app:\nGets deployed correctly Stays running Gets updated safely Survives when something crashes Scales like a beast (or kitten, if you ask nicely) In short, it’s a declarative way to manage ReplicaSets, which in turn manage Pods. You tell it what you want, and it handles the how.\nAnatomy of a Deployment Let’s look at a simple example and break it down:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: my-cool-app 5spec: 6 replicas: 3 7 selector: 8 matchLabels: 9 app: cool 10 template: 11 metadata: 12 labels: 13 app: cool 14 spec: 15 containers: 16 - name: cool-container 17 image: mycoolapp:latest 18 resources: 19 limits: 20 memory: \u0026#34;512Mi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 requests: 23 memory: \u0026#34;256Mi\u0026#34; 24 cpu: \u0026#34;250m\u0026#34; What’s happening here?\nreplicas: 3 - “Hey K8s, I want 3 copies of this Pod. No more, no less.” selector - Tells the Deployment how to find the Pods it manages. template - This is the actual Pod spec that ReplicaSets will use to launch Pods. resources - Requests and limits for CPU and memory. More on this in a sec. Labels and Selectors: The Kubernetes Dating App In Kubernetes, labels are like stickers on your containers: they tell the rest of the system what something is. Selectors are how components find each other.\nIn the Deployment above:\n1labels: 2 app: cool matches with:\n1selector: 2 matchLabels: 3 app: cool It’s basically saying: “I only manage Pods that are also into cool stuff.”\nNo label? No match. Kubernetes is strict like that.\nRequests and Limits: Keep Your Pods on a Diet Kubernetes is a multi-tenant system. Without resource limits, one greedy container could hog everything like it’s an all-you-can-eat buffet.\nRequests = The minimum CPU/memory a Pod needs to function. Limits = The maximum it’s allowed to consume. If your container goes beyond its limit, Kubernetes might throttle it. Or evict it. Or just silently judge you.\nAlways set these - they protect both your app and your cluster.\nA Bit More on ReplicaSets: The Middle Manager You Don’t See Every Deployment creates a ReplicaSet to do the actual work of launching Pods and keeping the correct number alive.\nAnd when you update your Deployment - like changing the image version - Kubernetes creates a new ReplicaSet for the new spec.\nYou can see them with:\n1kubectl get replicasets They’ll have names like my-cool-app-7fd56c9b89, with that funky hash showing it’s tied to a specific Pod spec.\nBonus Tip: Want to roll back your app to the previous version?\n1kubectl rollout undo deployment my-cool-app Because mistakes were made. And Kubernetes understands.\nUpdate Strategies: Because Downtime is So Last Decade When your app updates, Kubernetes doesn’t just slam everything shut and hope for the best. It uses update strategies to roll things out safely.\nRollingUpdate (Default) This strategy is like slowly replacing airplane engines mid-flight... and somehow it works.\n1strategy: 2 type: RollingUpdate 3 rollingUpdate: 4 maxUnavailable: 1 5 maxSurge: 1 maxUnavailable: 1 = Only one Pod can be down at a time. maxSurge: 1 = One extra Pod can be spun up temporarily. Use it when: Your app can run multiple versions at once. You want zero downtime. Your service can gracefully handle rolling changes. Example: Updating a Node.js REST API from v1.0 to v1.1? RollingUpdate is perfect.\nRecreate This is the “turn it off and on again” strategy.\n1strategy: 2 type: Recreate Kubernetes terminates all old Pods, then starts new ones. No overlap.\nUse it when: Only one version of the app can run at a time. Your app uses exclusive resources (like a local DB file). You’re in dev/test and just want a clean slate. Example: A monolithic app using a SQLite database. You can’t have two instances writing to it. Recreate is the way.\nWhat Happens When You Deploy? Let’s tie it all together:\nYou create a Deployment. Kubernetes creates a ReplicaSet. ReplicaSet launches Pods based on your template. K8s watches them like a hawk. If a Pod dies, it gets replaced. You update your Deployment - K8s creates a new ReplicaSet and does a rolling update. You drink coffee and take credit. Wrapping It Up (Without Wrapping Your Head Around It) Deployments are your way of telling Kubernetes: “Here’s what I want. Please keep it that way.” And Kubernetes obliges.\nWith Deployments, you get:\nAutomated scaling Self-healing Pods Easy rollbacks Safe rolling updates More time to do literally anything else So the next time someone says, “Just use a Deployment”, you can smile and say, “Already did, boss.”\n","link":"https://gavriliu.com/post/2025/04/17/kubernetes-deployments-like-a-boss-who-delegates-everything/","section":"post","tags":["kubernetes","learn","deployment"],"title":"Kubernetes Deployments: Like a Boss (Who Delegates Everything)"},{"body":"","link":"https://gavriliu.com/tags/gpg/","section":"tags","tags":null,"title":"Gpg"},{"body":"","link":"https://gavriliu.com/tags/images/","section":"tags","tags":null,"title":"Images"},{"body":"","link":"https://gavriliu.com/tags/skopeo/","section":"tags","tags":null,"title":"Skopeo"},{"body":"Ever found yourself needing to peek inside a container image without the hassle of pulling it? Or perhaps you wished for a smoother way to transfer images between registries without the Docker daemon breathing down your neck? Enter Skopeo — the command-line tool that lets you inspect, copy, and even sign container images, all without the need for a local container runtime.\nInstalling Skopeo macOS (via Homebrew) 1brew install skopeo Ubuntu 1sudo apt install skopeo For other distributions and detailed instructions, refer to the official installation guide.\nInspecting Images Without Pulling Curious about what’s inside an image but don’t want to download it? Skopeo’s inspect command has you covered:\n1skopeo inspect docker://docker.io/library/nginx:latest This command fetches metadata like layers, digests, and labels directly from the registry, saving you time and bandwidth. It’s like having X-ray vision for container images!\nWhen using skopeo inspect on macOS, you might encounter the following error:\n1FATA[0001] Error parsing manifest for image: choosing image instance: no image found in image index for architecture \u0026#34;arm64\u0026#34;, variant \u0026#34;v8\u0026#34;, OS \u0026#34;darwin\u0026#34; This occurs because the image you’re inspecting is a multi-architecture (multi-arch) image, and it doesn’t include a variant compatible with your system’s architecture and operating system. Skopeo attempts to select the appropriate image variant based on your current platform, and if it doesn’t find a match, it throws this error.\n1skopeo inspect \\ 2 --override-arch amd64 \\ 3 --override-os linux \\ 4 docker://docker.io/library/nginx:latest This command tells Skopeo to inspect the amd64 architecture variant for the linux operating system, bypassing the default behavior of matching your current platform.\n1{ 2 \u0026#34;Name\u0026#34;: \u0026#34;docker.io/library/nginx\u0026#34;, 3 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:09369da6b10306312cd908661320086bf87fbae1b6b0c49a1f50ba531fef2eab\u0026#34;, 4 \u0026#34;RepoTags\u0026#34;: [ 5 \u0026#34;1\u0026#34;, 6 \u0026#34;1-alpine\u0026#34;, 7 \u0026#34;1-alpine-otel\u0026#34;, 8 \u0026#34;1-alpine-perl\u0026#34;, 9 \u0026#34;1-alpine-slim\u0026#34;, 10 \u0026#34;1-alpine3.17\u0026#34;, 11 \u0026#34;1-alpine3.17-perl\u0026#34;, 12 \u0026#34;1-alpine3.17-slim\u0026#34;, 13 ... 14 \u0026#34;stable-bookworm\u0026#34;, 15 \u0026#34;stable-bookworm-otel\u0026#34;, 16 \u0026#34;stable-bookworm-perl\u0026#34;, 17 \u0026#34;stable-bullseye\u0026#34;, 18 \u0026#34;stable-bullseye-perl\u0026#34;, 19 \u0026#34;stable-otel\u0026#34;, 20 \u0026#34;stable-perl\u0026#34; 21 ], 22 \u0026#34;Created\u0026#34;: \u0026#34;2025-02-05T21:27:16Z\u0026#34;, 23 \u0026#34;DockerVersion\u0026#34;: \u0026#34;\u0026#34;, 24 \u0026#34;Labels\u0026#34;: { 25 \u0026#34;maintainer\u0026#34;: \u0026#34;NGINX Docker Maintainers \\u003cdocker-maint@nginx.com\\u003e\u0026#34; 26 }, 27 \u0026#34;Architecture\u0026#34;: \u0026#34;amd64\u0026#34;, 28 \u0026#34;Os\u0026#34;: \u0026#34;linux\u0026#34;, 29 \u0026#34;Layers\u0026#34;: [ 30 \u0026#34;sha256:8a628cdd7ccc83e90e5a95888fcb0ec24b991141176c515ad101f12d6433eb96\u0026#34;, 31 \u0026#34;sha256:75b6425929919354127c44985ea613fa508df8d80dbd1beafeb629400efb7541\u0026#34;, 32 \u0026#34;sha256:553c8756fd6670dc339ab500b042fe404386f114673f9c8af8dff3c6ade96cc7\u0026#34;, 33 \u0026#34;sha256:10fe6d2248e3ac5eab320a5c240e1aabfb0249d7b4b438b136633a8cbdc2190f\u0026#34;, 34 \u0026#34;sha256:3b6e18ae4ce61fa7b74c27a0b077d76bd53699d7e55b9e6a438c62282c0153e7\u0026#34;, 35 \u0026#34;sha256:3dce86e3b08256a60ab97ef86944f0c2a1e5c90a2df7806043c9969decfd82e8\u0026#34;, 36 \u0026#34;sha256:e81a6b82cf648bedba69393d4a1c09839203d02587537c8c9a7703c01b37af49\u0026#34; 37 ], 38 \u0026#34;LayersData\u0026#34;: [ 39 { 40 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 41 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:8a628cdd7ccc83e90e5a95888fcb0ec24b991141176c515ad101f12d6433eb96\u0026#34;, 42 \u0026#34;Size\u0026#34;: 28227259, 43 \u0026#34;Annotations\u0026#34;: null 44 }, 45 { 46 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 47 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:75b6425929919354127c44985ea613fa508df8d80dbd1beafeb629400efb7541\u0026#34;, 48 \u0026#34;Size\u0026#34;: 43954583, 49 \u0026#34;Annotations\u0026#34;: null 50 }, 51 { 52 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 53 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:553c8756fd6670dc339ab500b042fe404386f114673f9c8af8dff3c6ade96cc7\u0026#34;, 54 \u0026#34;Size\u0026#34;: 626, 55 \u0026#34;Annotations\u0026#34;: null 56 }, 57 { 58 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 59 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:10fe6d2248e3ac5eab320a5c240e1aabfb0249d7b4b438b136633a8cbdc2190f\u0026#34;, 60 \u0026#34;Size\u0026#34;: 952, 61 \u0026#34;Annotations\u0026#34;: null 62 }, 63 { 64 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 65 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:3b6e18ae4ce61fa7b74c27a0b077d76bd53699d7e55b9e6a438c62282c0153e7\u0026#34;, 66 \u0026#34;Size\u0026#34;: 399, 67 \u0026#34;Annotations\u0026#34;: null 68 }, 69 { 70 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 71 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:3dce86e3b08256a60ab97ef86944f0c2a1e5c90a2df7806043c9969decfd82e8\u0026#34;, 72 \u0026#34;Size\u0026#34;: 1210, 73 \u0026#34;Annotations\u0026#34;: null 74 }, 75 { 76 \u0026#34;MIMEType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, 77 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:e81a6b82cf648bedba69393d4a1c09839203d02587537c8c9a7703c01b37af49\u0026#34;, 78 \u0026#34;Size\u0026#34;: 1400, 79 \u0026#34;Annotations\u0026#34;: null 80 } 81 ], 82 \u0026#34;Env\u0026#34;: [ 83 \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, 84 \u0026#34;NGINX_VERSION=1.27.4\u0026#34;, 85 \u0026#34;NJS_VERSION=0.8.9\u0026#34;, 86 \u0026#34;NJS_RELEASE=1~bookworm\u0026#34;, 87 \u0026#34;PKG_RELEASE=1~bookworm\u0026#34;, 88 \u0026#34;DYNPKG_RELEASE=1~bookworm\u0026#34; 89 ] 90} Alternatively, if you want to view all available architecture variants for a multi-arch image, you can use the --raw flag to output the raw manifest:\n1skopeo inspect \\ 2 --raw docker://docker.io/library/nginx:latest | jq 1{ 2 \u0026#34;manifests\u0026#34;: [ 3 { 4 \u0026#34;annotations\u0026#34;: { 5 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;amd64\u0026#34;, 6 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:4b44499bc2a6c78d726f3b281e6798009c0ae1f034b0bfaf6a227147dcff928b\u0026#34;, 7 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 8 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:45:51Z\u0026#34;, 9 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 10 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 11 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 12 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 13 }, 14 \u0026#34;digest\u0026#34;: \u0026#34;sha256:b6653fca400812e81569f9be762ae315db685bc30b12ddcdc8616c63a227d3ca\u0026#34;, 15 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 16 \u0026#34;platform\u0026#34;: { 17 \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, 18 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; 19 }, 20 \u0026#34;size\u0026#34;: 2295 21 }, 22 { 23 \u0026#34;annotations\u0026#34;: { 24 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;amd64\u0026#34;, 25 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:b6653fca400812e81569f9be762ae315db685bc30b12ddcdc8616c63a227d3ca\u0026#34;, 26 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 27 }, 28 \u0026#34;digest\u0026#34;: \u0026#34;sha256:1fe8bc57e04d3add4c87bbc14f91569f392481a994ea47fc03b030a962a8a851\u0026#34;, 29 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 30 \u0026#34;platform\u0026#34;: { 31 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 32 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 33 }, 34 \u0026#34;size\u0026#34;: 841 35 }, 36 { 37 \u0026#34;annotations\u0026#34;: { 38 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm32v5\u0026#34;, 39 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:bf7ee0e80bc3f33bb693584818c1125cc4e6f1a2e12abfac2dd5fe3a0e0b9e73\u0026#34;, 40 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 41 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:52:17Z\u0026#34;, 42 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 43 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 44 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 45 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 46 }, 47 \u0026#34;digest\u0026#34;: \u0026#34;sha256:4bcba87eadc6ebb7b51ece2dadfb5493b1616ed5663cbfc6ac30e67dafabc44e\u0026#34;, 48 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 49 \u0026#34;platform\u0026#34;: { 50 \u0026#34;architecture\u0026#34;: \u0026#34;arm\u0026#34;, 51 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, 52 \u0026#34;variant\u0026#34;: \u0026#34;v5\u0026#34; 53 }, 54 \u0026#34;size\u0026#34;: 2297 55 }, 56 { 57 \u0026#34;annotations\u0026#34;: { 58 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm32v5\u0026#34;, 59 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:4bcba87eadc6ebb7b51ece2dadfb5493b1616ed5663cbfc6ac30e67dafabc44e\u0026#34;, 60 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 61 }, 62 \u0026#34;digest\u0026#34;: \u0026#34;sha256:e4b798256413dfb7b3b8c88d68d4b26b6c9e89e12c73059c9e01c8ef8645c3a3\u0026#34;, 63 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 64 \u0026#34;platform\u0026#34;: { 65 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 66 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 67 }, 68 \u0026#34;size\u0026#34;: 841 69 }, 70 { 71 \u0026#34;annotations\u0026#34;: { 72 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm32v7\u0026#34;, 73 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:8ba74c566fcc810a50be428d44244c386d27cb93c055480fca8c3f9acf002d08\u0026#34;, 74 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 75 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:51:32Z\u0026#34;, 76 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 77 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 78 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 79 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 80 }, 81 \u0026#34;digest\u0026#34;: \u0026#34;sha256:c50e8278e3d30be67e18be5cdf97280376dc90faf2b94ff99a214576aa7f6cd5\u0026#34;, 82 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 83 \u0026#34;platform\u0026#34;: { 84 \u0026#34;architecture\u0026#34;: \u0026#34;arm\u0026#34;, 85 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, 86 \u0026#34;variant\u0026#34;: \u0026#34;v7\u0026#34; 87 }, 88 \u0026#34;size\u0026#34;: 2297 89 }, 90 { 91 \u0026#34;annotations\u0026#34;: { 92 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm32v7\u0026#34;, 93 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:c50e8278e3d30be67e18be5cdf97280376dc90faf2b94ff99a214576aa7f6cd5\u0026#34;, 94 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 95 }, 96 \u0026#34;digest\u0026#34;: \u0026#34;sha256:fa8430d8826ed2d8107cca23a3246e7acc3d2b99369f5e6a66585447c2cf76e9\u0026#34;, 97 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 98 \u0026#34;platform\u0026#34;: { 99 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 100 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 101 }, 102 \u0026#34;size\u0026#34;: 841 103 }, 104 { 105 \u0026#34;annotations\u0026#34;: { 106 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm64v8\u0026#34;, 107 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:912d8a461ca5f85380a40de97d7b38dfcc39972de210518de07136126dd0bfa9\u0026#34;, 108 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 109 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T02:15:21Z\u0026#34;, 110 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 111 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 112 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 113 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 114 }, 115 \u0026#34;digest\u0026#34;: \u0026#34;sha256:846993cfd1ec2f814d7f3cfdc8df7aa67ecfe6ab233fd990c82d34eea47beb8e\u0026#34;, 116 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 117 \u0026#34;platform\u0026#34;: { 118 \u0026#34;architecture\u0026#34;: \u0026#34;arm64\u0026#34;, 119 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, 120 \u0026#34;variant\u0026#34;: \u0026#34;v8\u0026#34; 121 }, 122 \u0026#34;size\u0026#34;: 2297 123 }, 124 { 125 \u0026#34;annotations\u0026#34;: { 126 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;arm64v8\u0026#34;, 127 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:846993cfd1ec2f814d7f3cfdc8df7aa67ecfe6ab233fd990c82d34eea47beb8e\u0026#34;, 128 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 129 }, 130 \u0026#34;digest\u0026#34;: \u0026#34;sha256:41e936c6bb4d3fbcb5e0fd33db5287a78b5f6df40c455b0d9ed7337cd51cf209\u0026#34;, 131 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 132 \u0026#34;platform\u0026#34;: { 133 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 134 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 135 }, 136 \u0026#34;size\u0026#34;: 841 137 }, 138 { 139 \u0026#34;annotations\u0026#34;: { 140 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;i386\u0026#34;, 141 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:546fc136c89a1b0554689e33bfe000687048faddee5157844b374acc03d773fd\u0026#34;, 142 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 143 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:11:42Z\u0026#34;, 144 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 145 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 146 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 147 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 148 }, 149 \u0026#34;digest\u0026#34;: \u0026#34;sha256:31eefa9ba775ea648b8e5b9bd8ea834f3f0c86dbfb8513d34d6d6ae0645cc178\u0026#34;, 150 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 151 \u0026#34;platform\u0026#34;: { 152 \u0026#34;architecture\u0026#34;: \u0026#34;386\u0026#34;, 153 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; 154 }, 155 \u0026#34;size\u0026#34;: 2294 156 }, 157 { 158 \u0026#34;annotations\u0026#34;: { 159 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;i386\u0026#34;, 160 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:31eefa9ba775ea648b8e5b9bd8ea834f3f0c86dbfb8513d34d6d6ae0645cc178\u0026#34;, 161 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 162 }, 163 \u0026#34;digest\u0026#34;: \u0026#34;sha256:db10be2e0d2af5e97a58bf4317cc1534734f9aaf5c3bf071cf9fb0ec7b7c8a87\u0026#34;, 164 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 165 \u0026#34;platform\u0026#34;: { 166 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 167 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 168 }, 169 \u0026#34;size\u0026#34;: 841 170 }, 171 { 172 \u0026#34;annotations\u0026#34;: { 173 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;mips64le\u0026#34;, 174 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:32127f41084d3360f90315ef0c7c0deb43c73ee02382035c35ce978dad94b35d\u0026#34;, 175 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 176 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T02:21:58Z\u0026#34;, 177 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 178 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 179 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 180 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 181 }, 182 \u0026#34;digest\u0026#34;: \u0026#34;sha256:4a2475325e9bfd3afe7072826692a96265d3f8d21998f2053c1cfa4cb10ead7b\u0026#34;, 183 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 184 \u0026#34;platform\u0026#34;: { 185 \u0026#34;architecture\u0026#34;: \u0026#34;mips64le\u0026#34;, 186 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; 187 }, 188 \u0026#34;size\u0026#34;: 2298 189 }, 190 { 191 \u0026#34;annotations\u0026#34;: { 192 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;mips64le\u0026#34;, 193 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:4a2475325e9bfd3afe7072826692a96265d3f8d21998f2053c1cfa4cb10ead7b\u0026#34;, 194 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 195 }, 196 \u0026#34;digest\u0026#34;: \u0026#34;sha256:291f0ff112d6349f8e911e085ec08fd8573fd7cd2efe786dce2e2ec12edaa221\u0026#34;, 197 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 198 \u0026#34;platform\u0026#34;: { 199 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 200 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 201 }, 202 \u0026#34;size\u0026#34;: 567 203 }, 204 { 205 \u0026#34;annotations\u0026#34;: { 206 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;ppc64le\u0026#34;, 207 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:6d08d69167c094bb6f76c99167391ff6c35ee237eb48f2e3ea7fdb65608ce4d2\u0026#34;, 208 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 209 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:48:12Z\u0026#34;, 210 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 211 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 212 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 213 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 214 }, 215 \u0026#34;digest\u0026#34;: \u0026#34;sha256:fddb0f09b70b961b484c8694006384c04e245b4f8c35f28cc1e93ac89b34869d\u0026#34;, 216 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 217 \u0026#34;platform\u0026#34;: { 218 \u0026#34;architecture\u0026#34;: \u0026#34;ppc64le\u0026#34;, 219 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; 220 }, 221 \u0026#34;size\u0026#34;: 2297 222 }, 223 { 224 \u0026#34;annotations\u0026#34;: { 225 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;ppc64le\u0026#34;, 226 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:fddb0f09b70b961b484c8694006384c04e245b4f8c35f28cc1e93ac89b34869d\u0026#34;, 227 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 228 }, 229 \u0026#34;digest\u0026#34;: \u0026#34;sha256:634af8de5bdf18b1f6ff5614a6cc7a082d2390efa28f99f1dae67cab60713288\u0026#34;, 230 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 231 \u0026#34;platform\u0026#34;: { 232 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 233 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 234 }, 235 \u0026#34;size\u0026#34;: 841 236 }, 237 { 238 \u0026#34;annotations\u0026#34;: { 239 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;s390x\u0026#34;, 240 \u0026#34;org.opencontainers.image.base.digest\u0026#34;: \u0026#34;sha256:604d86fd7ef6bb1e6dc5709e4149cb81953d5f006aa41f376aef84c7f152f332\u0026#34;, 241 \u0026#34;org.opencontainers.image.base.name\u0026#34;: \u0026#34;debian:bookworm-slim\u0026#34;, 242 \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2025-04-08T01:48:17Z\u0026#34;, 243 \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;cffeb933620093bc0c08c0b28c3d5cbaec79d729\u0026#34;, 244 \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/nginxinc/docker-nginx.git#cffeb933620093bc0c08c0b28c3d5cbaec79d729:mainline/debian\u0026#34;, 245 \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://hub.docker.com/_/nginx\u0026#34;, 246 \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;1.27.4\u0026#34; 247 }, 248 \u0026#34;digest\u0026#34;: \u0026#34;sha256:a99b7d9812dbb7527eab8161807e879c3e66951ddccf21c5302061a651289da6\u0026#34;, 249 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 250 \u0026#34;platform\u0026#34;: { 251 \u0026#34;architecture\u0026#34;: \u0026#34;s390x\u0026#34;, 252 \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; 253 }, 254 \u0026#34;size\u0026#34;: 2295 255 }, 256 { 257 \u0026#34;annotations\u0026#34;: { 258 \u0026#34;com.docker.official-images.bashbrew.arch\u0026#34;: \u0026#34;s390x\u0026#34;, 259 \u0026#34;vnd.docker.reference.digest\u0026#34;: \u0026#34;sha256:a99b7d9812dbb7527eab8161807e879c3e66951ddccf21c5302061a651289da6\u0026#34;, 260 \u0026#34;vnd.docker.reference.type\u0026#34;: \u0026#34;attestation-manifest\u0026#34; 261 }, 262 \u0026#34;digest\u0026#34;: \u0026#34;sha256:a1e6f44bc7da1981130e18b36c1429d2a436a068642853d67ba36eaab952a2c6\u0026#34;, 263 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, 264 \u0026#34;platform\u0026#34;: { 265 \u0026#34;architecture\u0026#34;: \u0026#34;unknown\u0026#34;, 266 \u0026#34;os\u0026#34;: \u0026#34;unknown\u0026#34; 267 }, 268 \u0026#34;size\u0026#34;: 841 269 } 270 ], 271 \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.index.v1+json\u0026#34;, 272 \u0026#34;schemaVersion\u0026#34;: 2 273} This will display the manifest list, including all supported architectures and operating systems for the image. ￼\nBy using these options, you can effectively inspect and work with multi-architecture images, even when your current platform isn’t directly supported by the image’s manifest.\nCopying Images Between Registries Transferring images between registries can be a chore, especially when dealing with authentication and different formats. Skopeo simplifies this process:\n1skopeo copy \\ 2 docker://source.registry.com/myimage:latest \\ 3 docker://destination.registry.com/myimage:latest Need to handle credentials? No problem:\n1skopeo copy \\ 2 --dest-creds user:password \\ 3 docker://source.registry.com/myimage:latest \\ 4 docker://destination.registry.com/myimage:latest You might need to check other authentication flags depending on your setup.\nSigning Images for Enhanced Security In today’s security-conscious world, signing your container images is a best practice. Skopeo supports image signing using GPG:\nGenerate a GPG key: 1gpg --generate-key Follow the steps to generate, or use an existing one gpg -K\nSign the image during copy: 1skopeo copy \\ 2 --sign-by YOUR_KEY_ID \\ 3 docker://source.registry.com/myimage:latest \\ 4 docker://destination.registry.com/myimage:latest Verify a signed image 1skopeo inspect --verify-signatures docker://destination.registry.com/myimage:latest 1{ 2 \u0026#34;Name\u0026#34;: \u0026#34;destination.registry.com/myimage\u0026#34;, 3 \u0026#34;Digest\u0026#34;: \u0026#34;sha256:...\u0026#34;, 4 \u0026#34;RepoTags\u0026#34;: [ 5 \u0026#34;tag\u0026#34; 6 ], 7 \u0026#34;Created\u0026#34;: \u0026#34;2025-04-15T12:34:56Z\u0026#34;, 8 \u0026#34;DockerVersion\u0026#34;: \u0026#34;20.10.7\u0026#34;, 9 \u0026#34;Labels\u0026#34;: { 10 \u0026#34;maintainer\u0026#34;: \u0026#34;you@example.com\u0026#34; 11 }, 12 \u0026#34;Architecture\u0026#34;: \u0026#34;amd64\u0026#34;, 13 \u0026#34;Os\u0026#34;: \u0026#34;linux\u0026#34;, 14 \u0026#34;Layers\u0026#34;: [ 15 \u0026#34;sha256:...\u0026#34; 16 ] 17} If the verification fails, you would receive the following message:\n1FATA[0001] Signature for docker://destination.registry.com/myimage:latest is not valid: signature verification failed: no valid signatures found That’s about it! Skopeo is a powerful and flexible tool that can seriously level up your container workflows. Whether you’re automating image promotion across environments, verifying signatures for security, or just avoiding the hassle of pulling and pushing images manually — Skopeo has your back. It plays well in CI pipelines, helps keep registries clean and in sync, and makes inspecting remote images a breeze. Give it a spin, and you might wonder how you ever managed without it.\n","link":"https://gavriliu.com/post/2025/04/15/skopeo-the-container-image-swiss-army-knife-you-didn-t-know-you-needed/","section":"post","tags":["kubernetes","skopeo","images","gpg"],"title":"Skopeo: The Container Image Swiss Army Knife You Didn't Know You Needed"},{"body":"","link":"https://gavriliu.com/tags/apt/","section":"tags","tags":null,"title":"Apt"},{"body":"In an effort to balance power efficiency with my tendency to hoard old hardware, I decided to move my overpowered Pi-hole setup from a Mac Mini to a Raspberry Pi 2. (Yes, the Pi 2! A true relic from the past, but still kicking.)\nEverything was going smoothly — static IP, hostname set, network configured — until I ran the inevitable:\nThe problem 1apt update 1Hit:1 http://raspbian.raspberrypi.com/raspbian bookworm InRelease 2Hit:2 http://archive.raspberrypi.com/debian bookworm InRelease 3Reading package lists... Done 4Building dependency tree... Done 5Reading state information... Done 690 packages can be upgraded. Run \u0026#39;apt list --upgradable\u0026#39; to see them. 7W: http://raspbian.raspberrypi.com/raspbian/dists/bookworm/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. Weird, it's a brand new install, but anyway, let's fix it.\nThe fix Step 1: Find the Offending Key 1apt-key list | grep -A4 \u0026#34;trusted.gpg$\u0026#34; You'll get something like this:\n1Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)). 2/etc/apt/trusted.gpg 3-------------------- 4pub rsa2048 2012-04-01 [SC] 5 A0DA 38D0 D76E 8B5D 6388 7281 9165 938D 90FD DD2E 6uid [ unknown] Mike Thompson (Raspberry Pi Debian armhf ARMv6+VFP) \u0026lt;mpthompson@gmail.com\u0026gt; That long hex string at the bottom? That's our culprit. Grab the last 8 characters (e.g., 90FDDD2E).\nStep 2: Export the Key to a Temporary File 1apt-key export 90FDDD2E | sudo gpg --dearmor -o /tmp/raspberrypi-os.gpg If you're getting a warning about apt-key being deprecated — yes, we know. Thanks for the reminder, apt.\nStep 3: Verify the Exported Key\n1file /tmp/raspberrypi-os.gpg You should see something like:\n1/tmp/raspberrypi-os.gpg: PGP/GPG key public ring (v4) created Sun Jun 17 15:49:51 2012 RSA (Encrypt or Sign) 2048 bits MPI=0xabc2a41a70625f9f... If this checks out, we’re good to move on.\nStep 4: Delete the Old Key 1apt-key del 90FDDD2E Step 5: Move the Exported Key to the Right Place 1mv /tmp/raspberrypi-os.gpg /etc/apt/trusted.gpg.d/ The Moment of Truth Time to see if all this work actually made a difference.\n1apt update No errors? No legacy key warnings? Success!\nNow, for the real prize:\n1apt upgrade -y Watch those packages roll in, knowing you’ve successfully outwitted a deprecated key storage issue on a nearly decade-old Raspberry Pi.\nThis was one of those “why is this happening on a clean install?” moments that make homelabbing so much fun (and sometimes maddening). But at least now, my Pi-hole setup is running on a true low-power device, and I got a free lesson in Debian’s ever-evolving key management system.\nHope this helps someone else avoid the same head-scratching moment!\n","link":"https://gavriliu.com/post/2025/03/31/fix-key-stored-in-legacy-trusted-gpg-keyring/","section":"post","tags":["apt","gpg","keyring","legacy"],"title":"Fix Key Stored in Legacy Trusted Gpg Keyring"},{"body":"","link":"https://gavriliu.com/series/homelab/","section":"series","tags":null,"title":"Homelab"},{"body":"","link":"https://gavriliu.com/tags/keyring/","section":"tags","tags":null,"title":"Keyring"},{"body":"","link":"https://gavriliu.com/tags/legacy/","section":"tags","tags":null,"title":"Legacy"},{"body":"","link":"https://gavriliu.com/series/homeassistant/","section":"series","tags":null,"title":"Homeassistant"},{"body":"","link":"https://gavriliu.com/tags/homeassistant/","section":"tags","tags":null,"title":"Homeassistant"},{"body":"A few days ago, my wife decided that we - sorry, I — needed to rearrange our kid's room. This involved unplugging various things, including my IKEA VINDSTYRKA air quality sensor.\nFast forward to today: I noticed my zigbee2mqtt container had been restarting multiple times. Weird.\nBut everything seemed to be working fine — devices were updating, automations were triggering, life was good. Then I plugged the sensor back in, and... Home Assistant didn’t recognize it. Did I forget to pair it after playing around with it? Who knows. But no problem, right? Just press the button four times and let the magic happen.\nWell, magic didn't happen. Instead, my zigbee2mqtt container restarted. Uh-oh.\nThe Troubleshooting Rabbit Hole At this point, I had a feeling something was off. So I tried the usual troubleshooting steps:\nRestarted my SLZB-06M Zigbee coordinator (connected via Ethernet). Restarted the zigbee2mqtt and mqtt containers. Tried to rejoin the sensor again. Same result. Failure. Logs weren’t helpful, just cryptic messages from the Zigbee underworld.\n1Using \u0026#39;/app/data\u0026#39; as data directory 2Starting Zigbee2MQTT without watchdog. 3[2025-03-28 09:30:49] info: z2m: Logging to console, file (filename: log.log) 4[2025-03-28 09:30:49] info: z2m: Starting Zigbee2MQTT version 2.1.3 (commit #ba337bd329aeb4ca17735c0cf09b31293c8cff06) 5[2025-03-28 09:30:49] info: z2m: Starting zigbee-herdsman (3.2.7) 6[2025-03-28 09:30:49] info: zh:ember: Using default stack config. 7[2025-03-28 09:30:49] info: zh:ember: ======== Ember Adapter Starting ======== 8[2025-03-28 09:30:49] info: zh:ember:ezsp: ======== EZSP starting ======== 9[2025-03-28 09:30:49] info: zh:ember:uart:ash: ======== ASH Adapter reset ======== 10[2025-03-28 09:30:49] info: zh:ember:uart:ash: Socket ready 11[2025-03-28 09:30:49] info: zh:ember:uart:ash: ======== ASH starting ======== 12[2025-03-28 09:30:51] info: zh:ember:uart:ash: ======== ASH connected ======== 13[2025-03-28 09:30:51] info: zh:ember:uart:ash: ======== ASH started ======== 14[2025-03-28 09:30:51] info: zh:ember:ezsp: ======== EZSP started ======== 15[2025-03-28 09:30:51] info: zh:ember: Adapter EZSP protocol version (14) lower than Host. Switched. 16[2025-03-28 09:30:51] info: zh:ember: Adapter version info: {\u0026#34;ezsp\u0026#34;:14,\u0026#34;revision\u0026#34;:\u0026#34;8.0.2 [GA]\u0026#34;,\u0026#34;build\u0026#34;:397,\u0026#34;major\u0026#34;:8,\u0026#34;minor\u0026#34;:0,\u0026#34;patch\u0026#34;:2,\u0026#34;special\u0026#34;:0,\u0026#34;type\u0026#34;:170} 17[2025-03-28 09:30:51] info: zh:ember: [STACK STATUS] Network up. 18[2025-03-28 09:30:52] info: zh:ember: [INIT TC] Adapter network matches config. 19[2025-03-28 09:30:52] info: zh:ember: [CONCENTRATOR] Started source route discovery. 1246ms until next broadcast. 20[2025-03-28 09:30:52] info: z2m: zigbee-herdsman started (resumed) 21[...] 22[2025-03-28 09:22:37] info: zh:controller: Interview for \u0026#39;0xd44867fffe5b1c0c\u0026#39; started 23[2025-03-28 09:22:37] info: z2m: Device \u0026#39;0xd44867fffe5b1c0c\u0026#39; joined 24[2025-03-28 09:22:37] info: z2m: Starting interview of \u0026#39;0xd44867fffe5b1c0c\u0026#39; 25[2025-03-28 09:22:37] info: z2m:mqtt: MQTT publish: topic \u0026#39;zigbee2mqtt/bridge/event\u0026#39;, payload \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;friendly_name\u0026#34;:\u0026#34;0xd44867fffe5b1c0c\u0026#34;,\u0026#34;ieee_address\u0026#34;:\u0026#34;0xd44867fffe5b1c0c\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;device_joined\u0026#34;}\u0026#39; 26[2025-03-28 09:22:37] info: z2m:mqtt: MQTT publish: topic \u0026#39;zigbee2mqtt/bridge/event\u0026#39;, payload \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;friendly_name\u0026#34;:\u0026#34;0xd44867fffe5b1c0c\u0026#34;,\u0026#34;ieee_address\u0026#34;:\u0026#34;0xd44867fffe5b1c0c\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;started\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;device_interview\u0026#34;}\u0026#39; 27[2025-03-28 09:22:37] error: zh:ember:uart:ash: Received ERROR from adapter, with code=ERROR_EXCEEDED_MAXIMUM_ACK_TIMEOUT_COUNT. 28[2025-03-28 09:22:37] error: zh:ember:uart:ash: ASH disconnected | Adapter status: ASH_NCP_FATAL_ERROR 29[2025-03-28 09:22:37] error: zh:ember:uart:ash: Error while parsing received frame, status=ASH_NCP_FATAL_ERROR. 30[2025-03-28 09:22:37] error: zh:ember: Adapter fatal error: HOST_FATAL_ERROR 31[2025-03-28 09:22:37] info: zh:ember:uart:ash: ASH COUNTERS since last clear: 32[2025-03-28 09:22:37] info: zh:ember:uart:ash: Total frames: RX=92, TX=134 33[2025-03-28 09:22:37] info: zh:ember:uart:ash: Cancelled : RX=0, TX=0 34[2025-03-28 09:22:37] info: zh:ember:uart:ash: DATA frames : RX=72, TX=41 35[2025-03-28 09:22:37] info: zh:ember:uart:ash: DATA bytes : RX=1284, TX=501 36[2025-03-28 09:22:37] info: zh:ember:uart:ash: Retry frames: RX=17, TX=0 37[2025-03-28 09:22:37] info: zh:ember:uart:ash: ACK frames : RX=0, TX=91 38[2025-03-28 09:22:37] info: zh:ember:uart:ash: NAK frames : RX=0, TX=0 39[2025-03-28 09:22:37] info: zh:ember:uart:ash: nRdy frames : RX=0, TX=0 40[2025-03-28 09:22:37] info: zh:ember:uart:ash: CRC errors : RX=0 41[2025-03-28 09:22:37] info: zh:ember:uart:ash: Comm errors : RX=0 42[2025-03-28 09:22:37] info: zh:ember:uart:ash: Length \u0026lt; minimum: RX=0 43[2025-03-28 09:22:37] info: zh:ember:uart:ash: Length \u0026gt; maximum: RX=0 44[2025-03-28 09:22:37] info: zh:ember:uart:ash: Bad controls : RX=0 45[2025-03-28 09:22:37] info: zh:ember:uart:ash: Bad lengths : RX=0 46[2025-03-28 09:22:37] info: zh:ember:uart:ash: Bad ACK numbers : RX=0 47[2025-03-28 09:22:37] info: zh:ember:uart:ash: Out of buffers : RX=0 48[2025-03-28 09:22:37] info: zh:ember:uart:ash: Retry dupes : RX=17 49[2025-03-28 09:22:37] info: zh:ember:uart:ash: Out of sequence : RX=0 50[2025-03-28 09:22:37] info: zh:ember:uart:ash: ACK timeouts : RX=0 51[2025-03-28 09:22:37] error: zh:ember:uart:ash: Error while parsing received frame, status=ASH_NCP_FATAL_ERROR. 52[2025-03-28 09:22:37] info: zh:ember:uart:ash: ======== ASH stopped ======== 53[2025-03-28 09:22:37] info: zh:ember:ezsp: ======== EZSP stopped ======== 54[2025-03-28 09:22:37] info: zh:ember: ======== Ember Adapter Stopped ======== 55[2025-03-28 09:22:37] error: z2m: Adapter disconnected, stopping 56[2025-03-28 09:22:49] info: z2m:mqtt: MQTT publish: topic \u0026#39;zigbee2mqtt/bridge/state\u0026#39;, payload \u0026#39;{\u0026#34;state\u0026#34;:\u0026#34;offline\u0026#34;}\u0026#39; 57[2025-03-28 09:22:49] info: z2m: Disconnecting from MQTT server 58[2025-03-28 09:22:49] info: z2m: Stopping zigbee-herdsman... 59[2025-03-28 09:22:49] info: z2m: Stopped zigbee-herdsman 60[2025-03-28 09:22:49] info: z2m: Stopped Zigbee2MQTT Turning to the Internet (And Realizing No One Had a Fix) Like any responsible tech enthusiast, I turned to the Internet — GitHub issues, forums, random Reddit threads. And, as expected, I found discussions, but none of the solutions worked for me. Some people suggested downgrading firmware, others said to sacrifice a goat (or was it just reset the Zigbee network? Hard to tell).\nThe Fix That Worked Since nothing else was helping, I decided to go nuclear: flash the firmware. Here’s what I did:\nFlashed the Zigbee Router 20250220 firmware. Flashed the Zigbee Coordinator 20250220 firmware. And boom! Everything was back to normal. The sensor paired without issues, no more zigbee2mqtt restarts, and my smart home was happy again.\nLessons Learned Never underestimate the chaos a simple room rearrangement can cause. When in doubt, firmware flashing is sometimes the way to go. Maybe keep a backup Zigbee coordinator handy? (Future me, take note.) Hopefully, this helps someone avoid spending hours chasing ghost issues. If you're dealing with zigbee2mqtt restarting when adding a device, give firmware flashing a shot. Just, you know, be careful not to brick your hardware.\nHappy Zigbee-ing!\n","link":"https://gavriliu.com/post/2025/03/28/how-rearranging-a-kid-s-room-broke-my-zigbee-network-and-how-i-fixed-it/","section":"post","tags":["zigbee2mqtt","slzb-06m","troubleshoot","homeassistant"],"title":"How Rearranging a Kid's Room Broke My Zigbee Network (and How I Fixed It)"},{"body":"","link":"https://gavriliu.com/tags/slzb-06m/","section":"tags","tags":null,"title":"Slzb-06m"},{"body":"","link":"https://gavriliu.com/tags/troubleshoot/","section":"tags","tags":null,"title":"Troubleshoot"},{"body":"","link":"https://gavriliu.com/tags/zigbee2mqtt/","section":"tags","tags":null,"title":"Zigbee2mqtt"},{"body":"Sometimes, I totally forget to take out the garbage bins. And around here, the rule is simple: if your bins aren’t on the sidewalk, the garbage truck just cruises by like a VIP limo ignoring peasants.\nWhen we first moved in, I forgot to take out the recycling bags (Gelber Sack) three times in a row. This resulted in a majestic mountain of 25 bags filled with plastic, empty cat food cans, and my dignity.\nTo avoid turning my house into a landfill, I’m using the Waste Collection Schedule via HACS. Since I like keeping my data and my trash collection local, I downloaded the ICS file from the city’s waste management service and share it through a filebrowser container so Home Assistant can grab it.\nNow, I have three trusty sensor entities keeping me on track:\nsensor.restabfall sensor.gelber_sack sensor.papier configuration.yaml 1waste_collection_schedule: 2 sources: 3 - name: ics 4 args: 5 url: !secret waste_collection_ics_url 6 offset: 0 7 calendar_title: Waste Collection sensors.yaml 1- platform: waste_collection_schedule 2 name: Restabfall 3 value_template: \u0026#34;{% if value.daysTo == 0 %}Today{% elif value.daysTo == 1 %}Tomorrow{% else %}in {{value.daysTo}} days{% endif %}\u0026#34; 4 types: 5 - Restabfall 6- platform: waste_collection_schedule 7 name: Gelber Sack 8 value_template: \u0026#34;{% if value.daysTo == 0 %}Today{% elif value.daysTo == 1 %}Tomorrow{% else %}in {{value.daysTo}} days{% endif %}\u0026#34; 9 types: 10 - Gelber Sack 11- platform: waste_collection_schedule 12 name: Papier 13 value_template: \u0026#34;{% if value.daysTo == 0 %}Today{% elif value.daysTo == 1 %}Tomorrow{% else %}in {{value.daysTo}} days{% endif %}\u0026#34; 14 types: 15 - Papier Cards Armed with the mystical powers of card-mod, the sorcery of mushroom-entity-card, and the sheer chaos of a well-placed grid, I conjured up the following magical creation:\n1square: false 2columns: 3 3type: grid 4cards: 5 - type: custom:mushroom-entity-card 6 entity: sensor.restabfall 7 tap_action: 8 action: none 9 hold_action: 10 action: none 11 double_tap_action: 12 action: none 13 icon_type: none 14 fill_container: false 15 layout: vertical 16 card_mod: 17 style: | 18 ha-card { 19 background-color: 20 {% if is_state(\u0026#39;sensor.restabfall\u0026#39;, \u0026#39;in 2 days\u0026#39;) %} 21 #FFE4B5 22 {% elif is_state(\u0026#39;sensor.restabfall\u0026#39;, \u0026#39;Tomorrow\u0026#39;) %} 23 #FA8072 24 {% else %} 25 #FFFFFF 26 {% endif %} 27 } 28 - type: custom:mushroom-entity-card 29 entity: sensor.gelber_sack 30 tap_action: 31 action: none 32 icon_type: none 33 layout: vertical 34 hold_action: 35 action: none 36 double_tap_action: 37 action: none 38 card_mod: 39 style: | 40 ha-card { 41 background-color: 42 {% if is_state(\u0026#39;sensor.gelber_sack\u0026#39;, \u0026#39;in 2 days\u0026#39;) %} 43 #FFE4B5 44 {% elif is_state(\u0026#39;sensor.gelber_sack\u0026#39;, \u0026#39;Tomorrow\u0026#39;) %} 45 #FA8072 46 {% else %} 47 #FFFFFF 48 {% endif %} 49 } 50 - type: custom:mushroom-entity-card 51 entity: sensor.papier 52 tap_action: 53 action: none 54 icon_type: none 55 layout: vertical 56 hold_action: 57 action: none 58 double_tap_action: 59 action: none 60 card_mod: 61 style: | 62 ha-card { 63 background-color: 64 {% if is_state(\u0026#39;sensor.papier\u0026#39;, \u0026#39;in 2 days\u0026#39;) %} 65 #FFE4B5 66 {% elif is_state(\u0026#39;sensor.papier\u0026#39;, \u0026#39;Tomorrow\u0026#39;) %} 67 #FA8072 68 {% else %} 69 #FFFFFF 70 {% endif %} 71 } 72grid_options: 73 columns: 12 74 rows: auto ","link":"https://gavriliu.com/post/2025/03/14/waste-collection-schedule-in-ha/","section":"post","tags":["homeassistant","waste-collection"],"title":"Waste Collection Schedule in HomeAssistant"},{"body":"","link":"https://gavriliu.com/tags/waste-collection/","section":"tags","tags":null,"title":"Waste-Collection"},{"body":"Shoutout to derekbit for saving my cluster from a tragic post-power-outage existential crisis. My entire homelab went down for a few hours, and Longhorn wasn’t exactly in a hurry to come back. More details on the saga can be found here and in this lifesaving comment.\nTools used stern: Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. Check the logs To start questioning your life choices, run:\n1stern -n longhorn-system longhorn-manager This will flood your terminal with comforting messages like:\n1longhorn-manager-brgwp longhorn-manager E0129 16:59:30.764472 1 share_manager_controller.go:254] failed to sync longhorn-system/pvc-ca4a891c-cdc9-424e-949d-0ea016b80c84: pod share-manager-pvc-ca4a891c-cdc9-424e-949d-0ea016b80c84 for share manager not found 2longhorn-manager-brgwp longhorn-manager time=\u0026#34;2024-01-29T16:59:30Z\u0026#34; level=error msg=\u0026#34;Dropping Longhorn share manager out of the queue\u0026#34; func=controller.handleReconcileErrorLogging file=\u0026#34;utils.go:72\u0026#34; ShareManager=longhorn-system/pvc-7da9dfcf-a9b8-4995-ab1d-100a2a9ee72a controller=longhorn-share-manager error=\u0026#34;failed to sync longhorn-system/pvc-7da9dfcf-a9b8-4995-ab1d-100a2a9ee72a: pod share-manager-pvc-7da9dfcf-a9b8-4995-ab1d-100a2a9ee72a for share manager not found\u0026#34; node=hive02 In short: Longhorn was not having a good day.\nApply the fix Borrowing from the wisdom of the Issue thread, I decided to take the nuclear option and reset all my volumes:\n1for lhsm in $(kubectl -n longhorn-system get lhsm --no-headers | awk \u0026#39;{ print $1 }\u0026#39;) 2do 3 kubectl -n longhorn-system patch lhsm $lhsm --type=merge --subresource status --patch \u0026#39;status: {state: error}\u0026#39; 4 sleep 30 5done A few hours (and nervous sweats) later, everything was back to normal. No data loss, just a valuable lesson: Power outages are the devil, and Longhorn likes to hold grudges.\n","link":"https://gavriliu.com/post/2025/03/14/containers-not-mounting-longhorn-volumes/","section":"post","tags":["longhorn","troubleshooting"],"title":"Containers Not Mounting Longhorn Volumes"},{"body":"","link":"https://gavriliu.com/tags/longhorn/","section":"tags","tags":null,"title":"Longhorn"},{"body":"","link":"https://gavriliu.com/tags/troubleshooting/","section":"tags","tags":null,"title":"Troubleshooting"},{"body":"","link":"https://gavriliu.com/tags/application/","section":"tags","tags":null,"title":"Application"},{"body":"","link":"https://gavriliu.com/tags/argocd/","section":"tags","tags":null,"title":"Argocd"},{"body":"I feel pretty dumb right now. I used 2 ArgoCD applications for the same thing—one for my Helm chart, the other for flat manifest YAML files. Apparently, thinking isn’t my strong suit. I mean, I could barely remember where I put my coffee, let alone plan a proper ArgoCD setup!\nSo, like any reasonable person, I had to RTFM (read the fine manual, of course), and eventually stumbled upon the brilliant solution.\n1apiVersion: argoproj.io/v1alpha1 2kind: Application 3metadata: 4 name: kasten 5 namespace: argocd 6spec: 7 destination: 8 server: https://kubernetes.default.svc 9 namespace: kasten-io 10 project: default 11 sources: 12 - repoURL: https://charts.kasten.io 13 chart: k10 14 targetRevision: \u0026#34;7.5.7\u0026#34; 15 helm: 16 valueFiles: 17 - $values/clusters/hive/apps/kasten-io/helm/values.yaml 18 - repoURL: \u0026#39;git@github.com:AndreiGavriliu/homelab.git\u0026#39; 19 targetRevision: main 20 ref: values 21 - repoURL: \u0026#39;git@github.com:AndreiGavriliu/homelab.git\u0026#39; 22 targetRevision: main 23 path: clusters/hive/apps/kasten-io/k8s-manifests 24 syncPolicy: 25 syncOptions: 26 - CreateNamespace=true Look at how simple that was! How did I not think of this sooner? My brain must’ve been on vacation.\n","link":"https://gavriliu.com/post/2025/03/14/argocd-combine-helm-and-k8s-manifests-in-one-application/","section":"post","tags":["argocd","application","helm"],"title":"Combine Helm and K8s Manifests in One ArgoCD Application"},{"body":"","link":"https://gavriliu.com/tags/helm/","section":"tags","tags":null,"title":"Helm"},{"body":"","link":"https://gavriliu.com/tags/bitnami/","section":"tags","tags":null,"title":"Bitnami"},{"body":"I decided to push my Kubernetes manifests to GitHub because running Git locally on the same k3s cluster as ArgoCD seemed like asking for trouble. I mean, what happens if something goes wrong?\nSuddenly, you’re locked out of your repository, and your applications just sit there looking sad, like ‘Hey, I need a home!’ Pushing everything - including secrets — straight to GitHub? That’s like sending your passwords in a postcard: ‘Hey, here’s all my private stuff for the world to see!’ So, I decided to use Bitnami’s Sealed Secrets. At least now, my secrets are sealed tighter than my New Year’s resolution to work out!\nWhat are Bitnami’s Sealed Secrets? Problem: \u0026quot;I can manage all my K8s config in git, except Secrets.\u0026quot;\nSolution: Encrypt your Secret into a SealedSecret, which is safe to store - even inside a public repository. The SealedSecret can be decrypted only by the controller running in the target cluster and nobody else (not even the original author) is able to obtain the original Secret from the SealedSecret. (from the official project's GitHub Repository)\nYou might be wondering what it actually looks like\nHow to seal secrets Sensitive information For instance, if you need to seal the following credentials:\nPOSTGRES_HOST: 10.20.1.45 POSTGRES_PORT: 5432 POSTGRES_DB: fbi_undercover_agents POSTGRES_USER: admin POSTGRES_PASSWORD: superSecr3tPassw0rd Create a secret The values of each key in secret.yaml must be base64 encoded\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: fbi-agents 5 namespace: fbi 6data: 7 POSTGRES_HOST: MTAuMjAuMS40NQ== 8 POSTGRES_PORT: NTQzMg== 9 POSTGRES_DB: ZmJpX3VuZGVyY292ZXJfYWdlbnRz 10 POSTGRES_USER: YWRtaW4= 11 POSTGRES_PASSWORD: c3VwZXJTZWNyM3RQYXNzdzByZA== Seal the secret 1cat secret.yaml | kubeseal \\ 2 --controller-name=sealed-secrets \\ 3 --controller-namespace=sealed-secrets \\ 4 --format=yaml \\ 5 --scope=cluster-wide \u0026gt; sealed-secret.yaml --controller-name: Name of sealed-secrets controller. (default \u0026quot;sealed-secrets-controller\u0026quot;). Since I installed via helm, the controller name is sealed-secrets --controller-namespace: Namespace of sealed-secrets controller. (default \u0026quot;kube-system\u0026quot;) --format: Output format for sealed secret. Either json or yaml (default \u0026quot;json\u0026quot;) --scope: Set the scope of the sealed secret: strict, namespace-wide, cluster-wide (defaults to strict). Mandatory for --raw, otherwise the sealedsecrets.bitnami.com/cluster-wide and sealedsecrets.bitnami.com/namespace-wide annotations on the input secret can be used to select the scope. (default strict) This generates the file sealed-secret.yaml\nCheck the sealed secret file Check metadata.name, metadata.namespace, spec.template.metadata.name and spec.template.metadata.namespace are correct. Ensure these match your intended Secret deployment to avoid issues when applying the SealedSecret\n1apiVersion: bitnami.com/v1alpha1 2kind: SealedSecret 3metadata: 4 annotations: 5 sealedsecrets.bitnami.com/cluster-wide: \u0026#34;true\u0026#34; 6 creationTimestamp: null 7 name: fbi-agents 8 namespace: fbi 9spec: 10 encryptedData: 11 POSTGRES_DB: AgCimSDyG7/fDlP9f6lLjr1iVfL2UDcOYsrdkGAUipqIMO+Nd9/aVokfWVveKqAmF0ZFx1tDp2BO8YGv54A7oJVKCiLE3EYVMcsAa76Q3IrUtfbY3Qp5Kk9dLP0IuDn8zykZqpG4RSYqC4ZT+9hnaPRDp+BoeQysJJEoL5txyr/b39LbkUXmH/fuwbHT8ZmYZ7p1wkBxRVKL6vXnqO0GEYi9X8wxBw2ptQuvrt/JD9VUPByLoJTvqLXMfkGcnJ3Is6l5S54vU5Z9HQIodVSVBI34o1WUcP48K1Kj96MXvJ2UgremjmgNgFmrpc0T2cLsIIMAPhWj8g2KTDy2aB5zW2P48wVNW7Uu0foAP9rXgybPexSSA52Vv06ZYwED9zzVQJvvZlAt0Q78+7goB8MzlsU31R4LDs2FcKV1uxATKau+CKe+kKhAN/GBI+vLaLFHdmVIDdmNT3jIapoY00iFL9Xcrv7GdecWrMrov3ZC2co//1E2eZnb+aX71Jl9riYCNXP7J/TXlGc+LcQ4RPxQyWfJ8/r83MipMTfB+iqUjuvOTgVE/5Qf2HtU/1HPWOHgmGJTmiglyRYeLLzug45NBTeFimcIX/2JwqTJwF/sHcdqwu/iotNHNmYf1O/vbjIeWgHFVX8DqtZ/j/0G6zTWPBdEE0tcuKwwvGwYzhcQgj4SjVb9NoYpzkXZXH/p2NXHSnIIBcEynwsNphx71Q9FjtEkGCax5OQ= 12 POSTGRES_HOST: AgBQFZiZaZCeRVzWqAqOaxGuoSRAGPHmITd6XvW5K0NTvA8ZE+yrheKK4sQHS7htQQeFzBOEz99GrRxNLmhQbMDgA0GGd7qCm/kGsVEpNcf8C8yJ6KzRlCCyQdP9PcOIjwAvPxpisGJvStobcsxQXl9VWlCh/4VpkWED7JYJh8TlYd0XaXMR2i+6Q581C3cVPUH0V07EUPiPxstI4w18l8HqAfFqKZiT+uk9EsVXl/IPOqg9Dxg/SksrWuY7EP3R1lrubzQr+RM02LInw5cLh6iX9x3ZNiuF3BbnKKDgA5/tT1+qOP4bljE2JwHMOdo1oJZnrvw2ny4/x9nKOzkAa8KjogSTdFonDnlBV0bRXZ4G3RSZPNcp9l2TG5Q7itZNX1WcOMa+o5OJQUsVf6Y6aSTr+QB1LxeCk8tASPsu1HnaZSSzECr3fxMJ1gDRz6l8SFHLM9XQ0j9MN6QrnNSmWOUWnhAsa22uX+nYM43jGm6oELrlXo/fNLn0oPuY2xaKpO+5YNZ3AM+xGFFioPs+e2Hl+gBsXwY4ZmEou7vZ5/mSIVuTMBkmedi/sdhfpYmOwhVXfqsVa7Rh+ov6Ab6zOEC+xEnVmyxclesEY7wHonCNK4M4YiIXQalHo1M0mi4q78HvvXrOSzmqqRWBg3V8R8mwhypgg1aIgkx7Jti1HRqvXb9hg2rFU+R+fx3vnravYmNO4mt3ePs274GZ 13 POSTGRES_PASSWORD: AgCEGEVwbWu4GeNgqJDWPO+XgdsMj5j5vMguARiMLFIEz3asEqETItVqMoY+TaF4Cb0jQ51SuhpQYQwy/0LBJbriy9Nj/YxHEBg9q2M11kkd15S4Lwrnan44PiLKYwLjkpM5GkzHek60eT56MxeT2O4gX9muGfYvRle5W71eGba2JMktu8ERXnMStulOUf0Oqfx4DCD7YRgvhsdlFqBMLs9uAcefrXkWjXKuyJhkIdFJBIS9JLjbw68DyQXPWmwHTAbhXWetKIQHYAe+Yr//VZUolAfeqb7xW2NwcLr5WKrdbk9fHgxWKUwPP6KpcmlF2MlwJbntEzOF6ZYBy9IY1Gw4XEu+4ayT8KXYTK8y1gXzBgbFUINs0YNlJoxK7XMIcTj3gBCpg2e4cPHolBpi7bGhAuHJ5vh7d9HZ4KhhxXqcbQ6cin/V5zEK91okpFUyBtiZphlFWJTcgwnaqGbvuCTuh7XHYTEUKWaE+1ha4PV0qLqHzz9d6P2/lobYZyJ+xsczwrdaUyKz7/WgHlDk4Y/xy79ZL9xIOTyUYymEwrzgO2mSN3PAof87ryTM49cpQZd25BxtSa1Y/asYmd6FrpHdaQmxMndAPATugSp4t3XJoKpCVn7+JN4K5dEawAqJyKVqoB/8URcjHnPcgx/Fp4LOJ919WvmR/+wZpjx3eFnOC/+QOqE3H/DzPVhVwsOOUWWbktS2rHM0afy8YATqRI6W8Eja 14 POSTGRES_PORT: AgAOH0MigGUSP2Dh88jMdLV0aFURvlYNM9OChDHCeZSSELa0vnAR7f4rAMLQhO00EupMZj+EcVozXTY2ZoB8u1PKC0/WxjoHTvzHARwFHGMW/SjZFw4YXsBP8lIcFz0aHIHQcGE/vkF0KYPOgH3Gi9r7x3zT8bpHaDrSWtgcsw2Ijzx5bidBcvuGIPfi4R6bMrZxBhz1F+/AY360IVRwXxluCVoXmb+2X2Bp01XqAk2f6fh4dQDz9G+kDyvV7O0eW5KxLqmYHME+AzpEq+M7zE1xlPzE6vb1TB7DOJBBeeejGu/Iq3Yx26D8jAoSAMHsj4kPKEiYTwYZ0mbq6H789MYnnn/PIOkPHqo/2i6lHwJbxoAkGzIkBI6wKZGP5um1PCJFJv2n2DPMEDsTlxkL8j1l0TI607R00jJ5gc7f7Czepi+BXhyadn+wbnmEu/7yAEO13uKNa8M4DKcKUzuTFg/PghjLjK8PnUw14NdAcP8XjEvWtqtjo4QkPFsalAUmvEYZEODol0DiaG0jT50d8IYgXp6Hx5qlC1L1g0p3xPrO6QH8BVda8h/NePISB4rBo/PmQgvz1GyTRSgs14stt1RsjNAKzpNBACysCY0+MNpeAsqg8bLsGkBcl7bfac5HdrY45fR4lS6dIDLlLbW6htEWwBlDeOPX3HznSnnxDvf2ywdakpMzVLsLVII9qmcgDf4riTNo 15 POSTGRES_USER: AgBC8nVi5UKfNCOzMnzGm1GPafXu2TgAYHqlEfCyHD5DA8btqX8dWpbJehUuXhG39qJnrZe/f3vUwpUIWfgTma2+HUfBz71cyUgrrcvY0HczAvzqs8pz34KjeuZF5GD64rkcWgc0aP663F8ANGsFklsq3fsHzY2xH3dFiKukH6b2jLcitwfrhm55JcUtmnHfeGqwMYSj3MXwf49f07E84zdFH5PgImUOoA4KgzdlcnBzzyqutjSF8omTbTcAq4nmjO8Qa3+pN+gjxU5B2ATC6mtYdfO87DXEG60SasnTsWcS9vR9AKH8CpUX1VR/Q72KyJf5DNCSZsXUVm9kag6RC4KAH3P7xUX5OTlTHZkZsGUi0jI7wP6QtxSdQK9E8t7fHuvlBd7oEwdFYo6gOlLFanO8vHbGE1ZeJSYg3Fhl+bCevXnPRBU7xrqhnvg7VG/Ek9ctmjZHGGen/XrDa7cMpExjjVPuW6Ax+MFQmB+tryHwxDSJ0VTzFbax4yJK84iyvqmxai5xdedetAI6ZbQqcCZx/Hn8QfaKLAObEruA2ryBmyOIbo7Ymx5TLb0E8ZaAoPqW3Joa3dBd+OMhCEGZ0ts0CXKqBaIEEPs6rdz78afCd2WAFcCNJgT+KH596Z5OEfg9q3nD/Gs+WN4HKy+M20CkPo19kQlbcnIpTLNY5oSRGJSpvn9lovYvp6kN8ZQnlKtBmaDJfA== 16 template: 17 metadata: 18 annotations: 19 sealedsecrets.bitnami.com/cluster-wide: \u0026#34;true\u0026#34; 20 creationTimestamp: null 21 name: fbi-agents 22 namespace: fbi How to unseal secrets Download sealed secret key 1kubectl get secret \\ 2 -n sealed-secrets \\ 3 -l sealedsecrets.bitnami.com/sealed-secrets-key\u0026lt;uniqueId\u0026gt; \\ 4 -o yaml \u0026gt; sealed-secrets-key.yaml Unseal secret using downloaded key 1kubeseal --controller-name=sealed-secrets \\ 2 --controller-namespace=sealed-secrets \u0026lt; sealed-secret.yaml \\ 3 --recovery-unseal \\ 4 --recovery-private-key=sealed-secrets-key -o yaml ","link":"https://gavriliu.com/post/2025/03/14/how-to-seal-and-unseal-secrets-with-bitnami-sealed-secrets/","section":"post","tags":["bitnami","sealed-secrets","security"],"title":"How to seal and unseal secrets with Bitnami Sealed-Secrets"},{"body":"","link":"https://gavriliu.com/tags/sealed-secrets/","section":"tags","tags":null,"title":"Sealed-Secrets"},{"body":"","link":"https://gavriliu.com/tags/security/","section":"tags","tags":null,"title":"Security"},{"body":"Who? Andrei Gavriliu - Senior IT Consultant | OpenShift @ ConSol Consulting \u0026amp; Solutions Software GmbH\nI’ve been working at ConSol Consulting \u0026amp; Solutions Software GmbH since 2012, solving technical challenges and breaking things just enough to learn how to fix them again. I have a passion for technology, whether it’s bleeding-edge Kubernetes clusters or almost century-old bicycles that somehow still work better than modern ones.\nWhen I’m not in front of a terminal, I attempt downhill riding — with a strong emphasis on attempt — while trying to avoid turning myself into a cautionary tale. Homelabbing, rediscovering old tech, and tinkering with anything that has gears, or a slightly questionable user manual keep me busy.\nIf it’s complex, obscure, or slightly dangerous, I’m probably interested.\nI can’t think of any reason why, but if you want to get in touch with me or follow me, here I am:\nGitHub LinkedIn XING Why? I started this blog because I figured it’s time to share my knowledge and opinions with the world. After all, if it’s on the internet, it must be true… right? Well, apparently not. There’s a ton of outdated info and some outright nonsense out there. Even ChatGPT has its off days — sometimes the info is ancient, and other times it just hits me with the classic 'You’re right!' like I’m talking to a cheerleader instead of getting actual answers. So here I am, doing my part to make the internet a little less… well, internet-y!\nWhat? Here are some of the tools I rely on to keep my homelab running smoothly - no particular order, just pure utility. Each one helps me troubleshoot, automate, or manage my setup with less hassle. Because let’s be honest, a well-managed homelab means fewer late-night debugging sessions!\nansible, argocd, fluxcd, helm, k9s, kubectl, kubectx, kubens, kubeseal, oc, popeye, renovatebot, stern, tmux, velero, vscode\nPrivacy \u0026amp; Data Usage This website values your privacy and only uses local cookies to store your preferences (e.g.: light/dark mode). No third-party tracking cookies or external advertising networks are used.\nFor comments, the site integrates giscus (repository), a GitHub-based commenting system. To participate in discussions, you will need a GitHub account, as all comments are stored as discussions within my repository.\nFor analytics, we use a self-hosted Umami (repository) instance to gather insights on website traffic while respecting user privacy. No personal data is collected, and all analytics data remains within our infrastructure.\nBoth the static site and the analytics service are hosted in my self-managed homelab, ensuring full control over data and performance.\n","link":"https://gavriliu.com/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://gavriliu.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"Homelab My homelab is running on 4 Intel NUCs, all happily chugging along with k3s. The network backbone is a UniFi setup, featuring a UDMPro and a UniFi Switch 24 Pro POE. Right now, it’s a cozy 1GB network, but I’m planning to bump it up to 2.5GB soon—because, well, who doesn’t love faster speeds? The only thing slowing me down at the moment is the backup process—Longhorn and Kasten.io backups are giving me a bit of a bottleneck, with everything stored safely on my trusty old Synology DS918+.\nA few details about what makes it all work smoothly:\nTraefik (Dynamic ingress controller) ArgoCD (GitOps continuous delivery) Kasten.io (Kubernetes backup solution) Longhorn (Cloud-native storage) Bitnami Sealed Secrets (Encrypted secrets management) Graylog (Centralized log management) RenovateBot (Automated dependency updates) MetalLB (Load balancing for bare-metal) cert-manager (TLS certificate management) kube-prometheus-stack (Monitoring stack) Grafana (Data visualization) Prometheus (Metrics collection) AlertManager (Alert management) ... more details about the cluster and what Apps I am hosting, in my homelab series\nHome Assistant Home Assistant is like my home’s personal assistant, but instead of scheduling meetings, it’s reminding me to take the trash out and telling me when the washer’s done. It’s got some quirks, though—sometimes it seems to know me better than I know myself, like when it randomly turns on the lights at 3 AM and I’m left wondering if it’s just being helpful or trying to send me a message. At least it’s not sending passive-aggressive reminders… yet.\nA few integrations I use:\nAlarmo Google Calendar Homematic(IP) Local InfluxDB Nintendo Switch Parental Controls ONVIF OpenWeatherMap OpenWeatherMap History SMLIGHT Tasmota UniFi Network UniFi Protect Waze Travel Time ... more details about my Home Assistant instance and what Integrations I am using, in my homeassistant series\nTasmin I’m building Tasmin — a GitOps-like manager for your Tasmota devices that not only updates, configures, and backs them up, but also checks for changes in the config and corrects them when the devices finally decide to show up on the network. Think of it like TasmoAdmin and TasmoBackup, but with a ‘because I can’ attitude. Powered by FastAPI and a sprinkle of ‘I can do this better… probably’ energy!\nI’m too ashamed of my code right now, so the repository will be posted at a later date… after I make it look like I know what I’m doing.\n","link":"https://gavriliu.com/projects/","section":"","tags":null,"title":"Projects"}]